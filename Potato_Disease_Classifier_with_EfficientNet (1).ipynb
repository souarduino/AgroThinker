{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install efficientnet_pytorch\n",
        "!!pip install mlxtend --upgrade --no-deps\n",
        "#importing required modules\n",
        "import gdown\n",
        "import zipfile\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchsummary import summary\n",
        "from torchvision import datasets, transforms as T\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "import os\n",
        "import torch.nn.functional as F\n",
        "from tqdm.notebook import tqdm\n",
        "import torch.optim as optim\n",
        "from PIL import ImageFile\n",
        "from sklearn.metrics import accuracy_score\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oa34LtCKoQme",
        "outputId": "6609c7c1-f38e-4848-ddc7-059ab179b27d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: efficientnet_pytorch in /usr/local/lib/python3.7/dist-packages (0.7.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from efficientnet_pytorch) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->efficientnet_pytorch) (3.10.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#declaring batch size\n",
        "batch_size = 64\n",
        "lr=0.1\n",
        "epochs=10\n",
        "#applying required transformations on the dataset\n",
        "img_transforms = {\n",
        "    'train':\n",
        "    T.Compose([\n",
        "        T.Resize(size=(224,224)), \n",
        "        T.ToTensor(),\n",
        "        T.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]), \n",
        "        ]),\n",
        "\n",
        "    'valid':\n",
        "    T.Compose([\n",
        "        T.Resize(size=(224,224)),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "        ]),\n",
        "\n",
        "    'test':\n",
        "    T.Compose([\n",
        "        T.Resize(size=(224,224)),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "        ]),\n",
        "     }\n",
        "\n",
        "# creating Location of data: train, validation, test\n",
        "train_path=\"/content/drive/MyDrive/data_dir/Train\"\n",
        "valid_path=\"/content/drive/MyDrive/data_dir/Valid\"\n",
        "test_path=\"/content/drive/MyDrive/data_dir/Test\"\n",
        "\n",
        "\n",
        "# creating Datasets to each of  folder created in prev\n",
        "train_file=datasets.ImageFolder(train_path,transform=img_transforms['train'])\n",
        "valid_file=datasets.ImageFolder(valid_path,transform=img_transforms['valid'])\n",
        "test_file=datasets.ImageFolder(test_path,transform=img_transforms['test'])\n",
        "\n",
        "\n",
        "#Creating loaders for the dataset\n",
        "loaders_transfer={\n",
        "    'train':torch.utils.data.DataLoader(train_file,batch_size,shuffle=True),\n",
        "    'valid':torch.utils.data.DataLoader(valid_file,batch_size,shuffle=True),\n",
        "    'test': torch.utils.data.DataLoader(test_file,batch_size,shuffle=True)\n",
        "}"
      ],
      "metadata": {
        "id": "c_3oWdhtopbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#importing the pretrained EfficientNet model\n",
        "\n",
        "model = EfficientNet.from_pretrained('efficientnet-b7')\n",
        "\n",
        "# Freeze weights\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "in_features = model._fc.in_features\n",
        "\n",
        "\n",
        "# Defining Dense top layers after the convolutional layers\n",
        "model._fc = nn.Sequential(\n",
        "    nn.BatchNorm1d(num_features=in_features),    \n",
        "    nn.Linear(in_features, 512),\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm1d(512),\n",
        "    nn.Linear(512, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm1d(num_features=128),\n",
        "    nn.Dropout(0.4),\n",
        "    nn.Linear(128, 3),\n",
        "    )\n",
        "device= torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model =model.to(device)\n",
        "model"
      ],
      "metadata": {
        "id": "CKH8PMgPpY_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device) # move the model to GPU\n",
        "summary(model,input_size=(3,224,224))"
      ],
      "metadata": {
        "id": "mFaP8ZHuqcgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(y_pred,y_true):\n",
        "    y_pred = F.softmax(y_pred,dim = 1)\n",
        "    top_p,top_class = y_pred.topk(1,dim = 1)\n",
        "    equals = top_class == y_true.view(*top_class.shape)\n",
        "    return torch.mean(equals.type(torch.FloatTensor))"
      ],
      "metadata": {
        "id": "A9LZey1Qq2u5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class efficientNetTrainer():\n",
        "    \n",
        "    def __init__(self,criterion = None,optimizer = None,schedular = None):\n",
        "        \n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "        self.schedular = schedular\n",
        "    \n",
        "    def train_batch_loop(self,model,trainloader):\n",
        "        \n",
        "        train_loss = 0.0\n",
        "        train_acc = 0.0\n",
        "        \n",
        "        for images,labels in tqdm(trainloader): \n",
        "            \n",
        "            # move the data to CPU\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            logits = model(images)\n",
        "            loss = self.criterion(logits,labels)\n",
        "            \n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "            \n",
        "            train_loss += loss.item()\n",
        "            train_acc += accuracy(logits,labels)\n",
        "            \n",
        "        return train_loss / len(trainloader), train_acc / len(trainloader) \n",
        "\n",
        "    \n",
        "    def valid_batch_loop(self,model,validloader):\n",
        "        \n",
        "        valid_loss = 0.0\n",
        "        valid_acc = 0.0\n",
        "        \n",
        "        for images,labels in tqdm(validloader):\n",
        "            \n",
        "            # move the data to CPU\n",
        "            images = images.to(device) \n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            logits = model(images)\n",
        "            loss = self.criterion(logits,labels)\n",
        "            \n",
        "            valid_loss += loss.item()\n",
        "            valid_acc += accuracy(logits,labels)\n",
        "            \n",
        "        return valid_loss / len(validloader), valid_acc / len(validloader)\n",
        "            \n",
        "        \n",
        "    def fit(self,model,trainloader,validloader,epochs):\n",
        "        \n",
        "        valid_min_loss = np.Inf \n",
        "        \n",
        "        for i in range(epochs):\n",
        "            \n",
        "            model.train() # this turn on dropout\n",
        "            avg_train_loss, avg_train_acc = self.train_batch_loop(model,trainloader) ###\n",
        "            \n",
        "            model.eval()  # this turns off the dropout lapyer and batch norm\n",
        "            avg_valid_loss, avg_valid_acc = self.valid_batch_loop(model,validloader) ###\n",
        "            \n",
        "            if avg_valid_loss <= valid_min_loss :\n",
        "                print(\"Valid_loss decreased {} --> {}\".format(valid_min_loss,avg_valid_loss))\n",
        "                torch.save(model.state_dict(),'/content/efficientNet_potato3.pt')\n",
        "                valid_min_loss = avg_valid_loss\n",
        "\n",
        "                \n",
        "            print(\"Epoch : {} Train Loss : {:.6f} Train Acc : {:.6f}\".format(i+1, avg_train_loss, avg_train_acc))\n",
        "            print(\"Epoch : {} Valid Loss : {:.6f} Valid Acc : {:.6f}\".format(i+1, avg_valid_loss, avg_valid_acc))"
      ],
      "metadata": {
        "id": "OWU4HamrqjB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr =lr)\n",
        "trainer = efficientNetTrainer(criterion,optimizer)\n",
        "import time\n",
        "start = time.time()\n",
        "trainer.fit(model,loaders_transfer['train'],loaders_transfer['valid'],epochs = epochs)\n",
        "print(\"Total time: \", time.time() - start, \"seconds\")"
      ],
      "metadata": {
        "id": "5dCWBa7QrHQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('/content/efficientNet_potato3.pt'))\n",
        "all_preds = torch.tensor([])\n",
        "all_preds=all_preds.to(device)\n",
        "for idx,(image,label) in enumerate(test_file):\n",
        "    image,label = test_file[idx]\n",
        "    ps = model(image.to(device).unsqueeze(0))\n",
        "    ps = F.softmax(ps,dim = 1)\n",
        "    all_preds = torch.cat(\n",
        "            (all_preds, ps)\n",
        "            ,dim=0\n",
        "        )"
      ],
      "metadata": {
        "id": "v2G7xV2OscpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "targets =[s[1] for s in test_file.samples]\n",
        "targets=torch.tensor(targets)\n",
        "targets"
      ],
      "metadata": {
        "id": "y6G-KtB9ssGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "metadata": {
        "id": "laiqP8JAsvdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_num_correct(preds, labels):\n",
        "    return preds.argmax(dim=1).eq(labels).sum().item()"
      ],
      "metadata": {
        "id": "UcoPNieis0dQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred=get_num_correct(all_preds,targets.to(device))\n",
        "pred"
      ],
      "metadata": {
        "id": "aMIGF5bqs3jx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy=pred/len(test_file)\n",
        "accuracy"
      ],
      "metadata": {
        "id": "_cj2IXoCs56L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(targets.type(dtype=torch.int32).cpu(), all_preds.argmax(dim=1).type(dtype=torch.int32).cpu())\n",
        "cm"
      ],
      "metadata": {
        "id": "o4sHys68s8yH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = ['Early_Blight', 'Healthy', 'Late_Blight']\n",
        "fig, ax = plot_confusion_matrix(conf_mat=cm,\n",
        "                                colorbar=True,\n",
        "                                show_absolute=True,\n",
        "                                show_normed=True,\n",
        "                                class_names=labels)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "J9rGTQJzs_Vu"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Potato_Disease_Classifier_with_EfficientNet.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}